# Checks that env variables are set, that clusters are reachable
# Sets aws_region and {hub,spoke}_cluster_name and {hub,spoke}_fqdn facts
# Also sets aws_arn_userid and test_client_ec2_ssh_key_name

- name: Get AWS caller identity for Owner tag
  amazon.aws.aws_caller_info:
    profile: "{{ aws_profile }}"
  register: aws_caller_info

- name: Set AWS ARN User ID
  ansible.builtin.set_fact:
    aws_arn_userid: "{{ aws_caller_info.arn | split('/') | last }}"

- name: Set AWS ec2 ssh key name
  ansible.builtin.set_fact:
    test_client_ec2_ssh_key_name: "{{ aws_arn_userid }}-ssh-key-rsa"

- name: Set HUBCONFIG and SPOKECONFIG facts from env variables
  ansible.builtin.set_fact:
    HUBCONFIG: "{{ lookup('env', 'HUBCONFIG') }}"
    SPOKECONFIG: "{{ lookup('env', 'SPOKECONFIG') }}"

- name: Check for correct HUBCONFIG variable
  ansible.builtin.fail:
    msg: "HUBCONFIG variable needs to be set and pointing to the HUB kubeconfig file"
  when:
    HUBCONFIG is not defined or HUBCONFIG | length == 0

- name: Check for correct SPOKECONFIG env variable
  ansible.builtin.fail:
    msg: "SPOKECONFIG variable needs to be set and pointing to the REGION kubeconfig file"
  when:
    SPOKECONFIG is not defined or SPOKECONFIG | length == 0

- name: Show the two cluster kubeconfig paths
  ansible.builtin.debug:
    msg: "HUBCONFIG: {{ HUBCONFIG }} - SPOKECONFIG: {{ SPOKECONFIG }}"

- name: Check that both clusters are reachable
  ansible.builtin.shell: |
    oc cluster-info
  environment:
    KUBECONFIG: "{{ item }}"
  loop:
    - "{{ HUBCONFIG }}"
    - "{{ SPOKECONFIG }}"

- name: Get cluster routes and set facts
  ansible.builtin.shell: |
    oc get Ingress.config.openshift.io/cluster -o jsonpath='{.spec.domain}'
  environment:
    KUBECONFIG: "{{ item.config }}"
  register: route_result
  loop:
    - { name: "hub", config: "{{ HUBCONFIG }}" }
    - { name: "spoke", config: "{{ SPOKECONFIG }}" }

- name: Set cluster route info dynamically
  ansible.builtin.set_fact:
    "{{ item.item.name }}_cluster_name": "{{ (item.stdout | split('.'))[1] }}"
    "{{ item.item.name }}_fqdn": "{{ (item.stdout | split('.'))[1:] | join('.') }}"
  loop: "{{ route_result.results }}"        

- name: Get clusters' aws region
  ansible.builtin.shell: |
    oc get infrastructure cluster -o jsonpath='{.status.platformStatus.aws.region}'
  environment:
    KUBECONFIG: "{{ item.config }}"
  register: aws_region_result
  loop:
    - { name: "hub", config: "{{ HUBCONFIG }}" }
    - { name: "spoke", config: "{{ SPOKECONFIG }}" }

- name: Set cluster aws region info dynamically
  ansible.builtin.set_fact:
    "{{ item.item.name }}_aws_region": "{{ item.stdout }}"
  loop: "{{ aws_region_result.results }}"        

- name: Check if clusters are in different regions
  ansible.builtin.debug:
    msg: "WARNING: Hub cluster is in {{ hub_aws_region }}, spoke cluster is in {{ spoke_aws_region }}. Multi-region support is experimental."
  when: hub_aws_region != spoke_aws_region

- name: Validate clusters are in the same region (unless explicitly overridden)
  ansible.builtin.assert:
    that:
      - hub_aws_region == spoke_aws_region or allow_different_regions | default(false)
    fail_msg: |
      FATAL: Clusters are in different regions (hub: {{ hub_aws_region }}, spoke: {{ spoke_aws_region }}).
      Multi-region support is not fully implemented.
      If you want to proceed anyway, set 'allow_different_regions: true' in your overrides.yml file.
      WARNING: This may result in connectivity issues or deployment failures.

- name: Set aws region
  ansible.builtin.set_fact:
    aws_region: "{{ hub_aws_region }}"

- name: Print AWS infos
  ansible.builtin.debug:
    msg: "AWS User: {{ aws_arn_userid }} - AWS Profile: {{ aws_profile }}"
